{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tjBNrpoiggww"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AdrianoMedeirosPinheiro/CNN.git"
      ],
      "metadata": {
        "id": "iEWGQVuGM3xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Crie um novo diretório para extrair o arquivo zipado\n",
        "!mkdir dataset\n",
        "!mkdir DataSet\n",
        "\n",
        "# Extraia o conteúdo do arquivo zipado no novo diretório\n",
        "with zipfile.ZipFile('/content/CNN/leopardo.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('dataset')\n",
        "\n",
        "with zipfile.ZipFile('/content/CNN/onca.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('dataset')\n"
      ],
      "metadata": {
        "id": "NG-KHA4QNnsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redimensionando as Imagens"
      ],
      "metadata": {
        "id": "2Ek9a6jZgpyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Dimensões da imagem redimensionada\n",
        "width = 100\n",
        "height = 100\n",
        "\n",
        "# Diretório com as imagens originais\n",
        "original_dir = '/content/dataset'\n",
        "\n",
        "# Diretório para salvar as imagens redimensionadas\n",
        "resized_dir = '/content/DataSet'\n",
        "\n",
        "# Cria o diretório para salvar as imagens redimensionadas, se ele não existir\n",
        "if not os.path.exists(resized_dir):\n",
        "    os.makedirs(resized_dir)\n",
        "\n",
        "# Percorre todas as imagens no diretório original\n",
        "for file in os.listdir(original_dir):\n",
        "      \n",
        "# Abre a imagem original\n",
        "  with Image.open(os.path.join(original_dir, file)) as im:\n",
        "        \n",
        "       \n",
        "# Redimensiona a imagem\n",
        "        im = im.resize((width, height))\n",
        "  \n",
        "# Salva a imagem redimensionada no diretório de destino\n",
        "        im.save(os.path.join(resized_dir, file))"
      ],
      "metadata": {
        "id": "ymFZ15mWS-_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET"
      ],
      "metadata": {
        "id": "f-VVfdFMcLxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import image\n",
        "\n",
        "\n",
        "# Caminho para a pasta com as imagens\n",
        "path = '/content/DataSet'\n",
        "\n",
        "# Inicializa um array vazio para armazenar as imagens\n",
        "images = []\n",
        "\n",
        "# Percorre todos os arquivos na pasta\n",
        "for file in os.listdir(path):\n",
        "    # Lê a imagem\n",
        "    img = image.imread(os.path.join(path, file))\n",
        "    # Adiciona a imagem ao array\n",
        "    images.append(img)\n",
        "\n",
        "# Converte o array de imagens em um array NumPy\n",
        "images = np.array(images)"
      ],
      "metadata": {
        "id": "n1Co7b3NijWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape\n"
      ],
      "metadata": {
        "id": "KPPiH1W0kRoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classe = []\n",
        "\n",
        "for i in range(1, 41):\n",
        "  classe.append(0)\n",
        "\n",
        "for i in range(41, 81):\n",
        "  classe.append(1)  \n",
        "\n",
        "classe_array = np.asarray(classe)"
      ],
      "metadata": {
        "id": "BP9G6oXiQE2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(images, classe_array, train_size=0.8)"
      ],
      "metadata": {
        "id": "lz33RM4YlMyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "9CP8xXd-7F_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_test.shape\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "vvNAGZbjlgzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(64, 100, 100, 3)\n",
        "X_test = X_test.reshape(16, 100, 100, 3)"
      ],
      "metadata": {
        "id": "RDn_YR8qxHAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "7UkzYXQxx5eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "zpO1v4e7luPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rede Neural\n"
      ],
      "metadata": {
        "id": "xwHzvsvxqie8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXiuZulIguyf"
      },
      "source": [
        "## Construindo a Rede Neural Convolucional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFTetk8ngy0f"
      },
      "source": [
        "### Definindo o modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "UBBuCHTOFbht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TR0JGP5gq2i"
      },
      "source": [
        "model = tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j92d4FE0hTZV"
      },
      "source": [
        "### Adicionado a primeira camada de convolução\n",
        "\n",
        "Hyper-parâmetros da camada de convolução:\n",
        "- filters (filtros): 32\n",
        "- kernel_size (tamanho do kernel): 3\n",
        "- padding (preenchimento): valid\n",
        "- função de ativação: relu\n",
        "- input_shape (camada de entrada): (100, 100, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSkL1iOvg_dE"
      },
      "source": [
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"valid\", activation=\"relu\", input_shape=(100,100,3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfRAaRmWiSlZ"
      },
      "source": [
        "### Adicionando a segunda camada de convolução e a camada de max-pooling\n",
        "\n",
        "Hyper-parâmetros da camada de convolução:\n",
        "- filters (filtros): 32\n",
        "- kernel_size (tamanho do kernel):3\n",
        "- padding (preenchimento): valid\n",
        "- função de ativação: relu\n",
        "\n",
        "Hyper-parâmetros da camada de max-pooling:\n",
        "- pool_size: 2\n",
        "- strides: 2\n",
        "- padding: valid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSl7Es5yidMp"
      },
      "source": [
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"valid\", activation=\"relu\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmP9h5wliAR6"
      },
      "source": [
        "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd8ERDyvin-0"
      },
      "source": [
        "### Adicionando a terceira camada de convolução\n",
        "\n",
        "Hyper-parâmetros da camada de convolução:\n",
        "\n",
        "    filters: 64\n",
        "    kernel_size:3\n",
        "    padding: valid\n",
        "    activation: relu\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9HWy6aFixEw"
      },
      "source": [
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"valid\", activation=\"relu\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O55kyOQGi44V"
      },
      "source": [
        "###  Adicionando a quarta camada de convolução e a camada de max pooling\n",
        "\n",
        "Hyper-parâmetros da camada de convolução:\n",
        "\n",
        "    filters: 64\n",
        "    kernel_size:3\n",
        "    padding: valid\n",
        "    activation: relu\n",
        "\n",
        "Hyper-parâmetros da camada de max pooling:\n",
        "\n",
        "    pool_size: 2\n",
        "    strides: 2\n",
        "    padding: valid\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b7vAuhjjCF2"
      },
      "source": [
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"valid\", activation=\"relu\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc493G2BjFhg"
      },
      "source": [
        "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hGnR3aXjKbZ"
      },
      "source": [
        "### Adicionando a camada de flattening"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLzu2cCVjI5Z"
      },
      "source": [
        "model.add(tf.keras.layers.Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpeRUvVWjR1W"
      },
      "source": [
        "### Adicionando a primeira camada densa (fully-connected)\n",
        "\n",
        "Hyper-parâmetros da camada densa:\n",
        "- units/neurônios: 128\n",
        "- função de ativação: relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWzYY8kKjhnZ"
      },
      "source": [
        "model.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaakKTqRjrkF"
      },
      "source": [
        "### Adicionando a camada de saída\n",
        "\n",
        "Hyper-parâmetros da camada de saída:\n",
        "\n",
        " - units/neurônios: 10 (número de classes)\n",
        " - activation: softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t-JmzRvjnBj"
      },
      "source": [
        "model.add(tf.keras.layers.Dense(units=2, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRr3bCU-ti06"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYgvbNihtprw"
      },
      "source": [
        "### Compilando o modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYHELxz4tsa-"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gox3SmwUtwgX"
      },
      "source": [
        "### Treinando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "historico = model.fit(X_train, y_train, batch_size=64, epochs=25, validation_data=(X_test, y_test), verbose=1)"
      ],
      "metadata": {
        "id": "IMJW1qzGZESh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8C7Pm0NuOrJ"
      },
      "source": [
        "### Avaliando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9r8TtNet3D0"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rpAPpfzuV0p"
      },
      "source": [
        "print(\"Test accuracy: {}\".format(test_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp7HNHS7I0HU"
      },
      "source": [
        "test_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acuracia_treino = historico.history['accuracy']\n",
        "acuracia_teste = historico.history['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(acuracia_treino)+1) # x do gráfico\n",
        "\n",
        "plt.plot(epochs, acuracia_treino, '-g', label='Acurácia Dados De Treino' )\n",
        "plt.plot(epochs, acuracia_teste, '-b', label='Acurácia Dados De Teste' )\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "W1y3Jn1mWFns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predição"
      ],
      "metadata": {
        "id": "tjBNrpoiggww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.utils as image\n",
        "#from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from numpy import expand_dims, argmax"
      ],
      "metadata": {
        "id": "a8NlaVSccDm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import expand_dims, argmax\n",
        "imgW, imgH = 100, 100\n",
        "\n",
        "file = '/content/leopardo (1).jpg'\n",
        "\n",
        "with Image.open(os.path.join(original_dir, file)) as im:\n",
        "\n",
        "  im = im.resize((width, height))\n",
        "\n",
        "y = model.predict(expand_dims(\n",
        "image.img_to_array(im), axis = 0))\n",
        "\n",
        "print('Animal in:', im, ':', classes[argmax(y)])"
      ],
      "metadata": {
        "id": "FSn1sNWzYBd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Otimização"
      ],
      "metadata": {
        "id": "vmIQNBM7YO_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "\n",
        "# cria o modelo da rede neural\n",
        "def create_model(num_filters, num_filters_2, kernel_size, pool_size, hidden_size):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(num_filters, kernel_size, padding='valid', activation=\"relu\", input_shape=(100,100,3)))\n",
        "  model.add(Conv2D(num_filters, kernel_size, padding='valid', activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size, strides=2, padding='valid'))\n",
        "  model.add(Conv2D(num_filters_2, kernel_size, padding='valid', activation=\"relu\"))\n",
        "  model.add(Conv2D(num_filters_2, kernel_size, padding='valid', activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size, strides=2, padding='valid'))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(hidden_size, activation='relu'))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# cria o modelo embutido na classe KerasClassifier\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# define os parâmetros que serão otimizados\n",
        "param_grid = {\n",
        "    'num_filters': [32, 64, 128],\n",
        "    'num_filters_2': [32, 64, 128],\n",
        "    'kernel_size': [3, 5, 7],\n",
        "    'pool_size': [2, 4, 6],\n",
        "    'hidden_size': [128, 200, 250]\n",
        "}\n",
        "\n",
        "# cria o objeto de pesquisa em grade\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
        "\n",
        "# ajusta o modelo usando os dados de treinamento e os parâmetros otimizados\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# mostra os melhores parâmetros encontrados durante a otimização\n",
        "print(\"Melhores parâmetros: {}\".format(grid_result.best_params_))\n",
        "\n",
        "# mostra a acurácia média para os melhores parâmetros\n",
        "print(\"Acurácia média: {:.2f}%\".format(grid_result.best_score_ * 100))\n"
      ],
      "metadata": {
        "id": "5irYrrhV656k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resnet"
      ],
      "metadata": {
        "id": "HJuVcJuPBwvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primeiro, precisamos instalar as bibliotecas necessárias\n",
        "!pip install torchvision\n",
        "\n",
        "# Em seguida, importamos as bibliotecas que vamos precisar\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Carregamos o modelo ResNet pré-treinado\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# Desativamos os gradientes no modelo pré-treinado\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Adicionamos uma camada linear no final do modelo pré-treinado\n",
        "# para adaptá-lo às nossas classes personalizadas\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, len(classe_array))\n",
        "\n",
        "# Definimos os parâmetros de treinamento\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Definimos o otimizador e a função de perda\n",
        "optimizer = torch.optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "custom_train_dataloader = X_train\n",
        "\n",
        "# Iteramos sobre os epochs\n",
        "for epoch in range(num_epochs):\n",
        "  # Iteramos sobre as imagens do conjunto de treinamento\n",
        "  for images, labels in custom_train_dataloader:\n",
        "    # Colocamos as imagens e rótulos no dispositivo apropriado (GPU ou CPU)\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Zeramos os gradientes do otimizador\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Executamos o modelo e calculamos a perda\n",
        "    outputs = model(images)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "\n",
        "    # Realizamos o backpropagation e otimização\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # Calculamos a precisão do conjunto de treinamento no final de cada epoch\n",
        "  train_acc = calculate_accuracy(model, custom_train_dataloader, device)\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs}: Training accuracy = {train_acc:.4f}\")\n",
        "\n",
        "# Calculamos a precisão do conjunto de teste no final do treinamento\n",
        "test_acc = calculate_accuracy(model, custom_test_dataloader, device)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "YZuUwx4LB1FW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}